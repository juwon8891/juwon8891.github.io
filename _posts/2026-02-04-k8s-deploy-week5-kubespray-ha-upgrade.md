---
layout: post
title: "[K8s-Deploy] Week 5 - Kubespray HA & Upgrade"
date: 2026-02-04
categories: [K8s-Deploy, Kubernetes, Kubespray, HA, LoadBalancing, Upgrade, Ansible]
---

# [K8s-Deploy] Week 5 - Kubespray HA & Upgrade

> **ê³ ê°€ìš©ì„± Kubernetes í´ëŸ¬ìŠ¤í„° ìš´ì˜**: Kubesprayë¥¼ í™œìš©í•œ HA êµ¬ì„±, API ì—”ë“œí¬ì¸íŠ¸ ì „ëµ, ë…¸ë“œ ê´€ë¦¬ ë° í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ğŸ¯ Week 5 í•™ìŠµ ëª©í‘œ](#-week-5-í•™ìŠµ-ëª©í‘œ)
   - [í•µì‹¬ ì£¼ì œ](#1-í•µì‹¬-ì£¼ì œ)
   - [ì‹¤ìŠµ í™˜ê²½](#2-ì‹¤ìŠµ-í™˜ê²½)

2. [ğŸ—ï¸ ì‹¤ìŠµ í™˜ê²½ êµ¬ì„±](#ï¸-ì‹¤ìŠµ-í™˜ê²½-êµ¬ì„±)
   - [ê°€ìƒë¨¸ì‹  êµ¬ì„±](#1-ê°€ìƒë¨¸ì‹ -êµ¬ì„±)
   - [ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„](#2-ì´ˆê¸°í™”-ìŠ¤í¬ë¦½íŠ¸-ë¶„ì„)
   - [Kubespray ë°°í¬](#3-kubespray-ë°°í¬)

3. [ğŸŒ K8S API ì—”ë“œí¬ì¸íŠ¸ ì „ëµ](#-k8s-api-ì—”ë“œí¬ì¸íŠ¸-ì „ëµ)
   - [Case 1: Client-Side LoadBalancing](#case-1-client-side-loadbalancing)
   - [Case 2: External LB + Client-Side LB](#case-2-external-lb--client-side-lb)
   - [Case 3: External LB Only](#case-3-external-lb-only)

4. [ğŸ”§ ë…¸ë“œ ê´€ë¦¬](#-ë…¸ë“œ-ê´€ë¦¬)
   - [ë…¸ë“œ ì¶”ê°€ (scale.yml)](#1-ë…¸ë“œ-ì¶”ê°€-scaleyml)
   - [ë…¸ë“œ ì œê±° (remove-node.yml)](#2-ë…¸ë“œ-ì œê±°-remove-nodeyml)
   - [ë¹„ì •ìƒ ë…¸ë“œ ê°•ì œ ì‚­ì œ](#3-ë¹„ì •ìƒ-ë…¸ë“œ-ê°•ì œ-ì‚­ì œ)
   - [í´ëŸ¬ìŠ¤í„° ë¦¬ì…‹ (reset.yml)](#4-í´ëŸ¬ìŠ¤í„°-ë¦¬ì…‹-resetyml)

5. [ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •](#-ëª¨ë‹ˆí„°ë§-ì„¤ì •)
   - [kube-ops-view ì„¤ì¹˜](#1-kube-ops-view-ì„¤ì¹˜)
   - [HAProxy í†µê³„ í˜ì´ì§€](#2-haproxy-í†µê³„-í˜ì´ì§€)

6. [ğŸ’¡ í•µì‹¬ ê°œë… ì •ë¦¬](#-í•µì‹¬-ê°œë…-ì •ë¦¬)
   - [Client-Side vs External LoadBalancing](#1-client-side-vs-external-loadbalancing)
   - [Kubespray ë³€ìˆ˜ ìš°ì„ ìˆœìœ„](#2-kubespray-ë³€ìˆ˜-ìš°ì„ ìˆœìœ„)
   - [etcd Deployment Type](#3-etcd-deployment-type)
   - [PodDisruptionBudgetê³¼ Drain](#4-poddisruptionbudgetê³¼-drain)

7. [ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
   - [ì¸ì¦ì„œ SAN ì¶”ê°€](#1-ì¸ì¦ì„œ-san-ì¶”ê°€)
   - [Containerd rlimits ì´ìŠˆ](#2-containerd-rlimits-ì´ìŠˆ)
   - [PDBë¡œ ì¸í•œ Drain ì‹¤íŒ¨](#3-pdbë¡œ-ì¸í•œ-drain-ì‹¤íŒ¨)

8. [ğŸ“ Week 5 í•™ìŠµ ì •ë¦¬](#-week-5-í•™ìŠµ-ì •ë¦¬)

9. [ğŸ“š ì°¸ê³  ìë£Œ](#-ì°¸ê³ -ìë£Œ)

---

## ğŸ¯ Week 5 í•™ìŠµ ëª©í‘œ

### 1. í•µì‹¬ ì£¼ì œ

Week 5ì—ì„œëŠ” **Kubesprayë¥¼ í™œìš©í•œ ê³ ê°€ìš©ì„±(HA) Kubernetes í´ëŸ¬ìŠ¤í„° êµ¬ì„± ë° ìš´ì˜**ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸**:
- âœ… **HA í´ëŸ¬ìŠ¤í„° êµ¬ì„±**: 3-Node Control Plane, 2-Node Worker
- âœ… **API ì—”ë“œí¬ì¸íŠ¸ ì „ëµ**: Client-Side LB vs External LB
- âœ… **ë…¸ë“œ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬**: ì¶”ê°€(scale), ì œê±°(remove-node), ë¦¬ì…‹(reset)
- âœ… **ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤**: Control Plane ë…¸ë“œ ì¥ì•  ì‹œ ë™ì‘ í™•ì¸
- âœ… **LoadBalancing**: HAProxy, Nginx, Kube-VIP ë¹„êµ
- âœ… **ëª¨ë‹ˆí„°ë§**: kube-ops-view, HAProxy í†µê³„

### 2. ì‹¤ìŠµ í™˜ê²½

**ê°€ìƒë¨¸ì‹  êµ¬ì„±** (Case2: External LB + HA Control Plane):

| í˜¸ìŠ¤íŠ¸ëª… | ì—­í•  | CPU | RAM | IP ì£¼ì†Œ |
|----------|------|-----|-----|----------|
| admin-lb | Kubespray ì‹¤í–‰, API LB | 2 | 1GB | 192.168.10.10 |
| k8s-node1 | Control Plane | 4 | 2GB | 192.168.10.11 |
| k8s-node2 | Control Plane | 4 | 2GB | 192.168.10.12 |
| k8s-node3 | Control Plane | 4 | 2GB | 192.168.10.13 |
| k8s-node4 | Worker | 4 | 2GB | 192.168.10.14 |
| k8s-node5 | Worker | 4 | 2GB | 192.168.10.15 |

**ì†Œí”„íŠ¸ì›¨ì–´ ë²„ì „**:
- **OS**: Rocky Linux 10.0
- **Kubernetes**: v1.32.9
- **Kubespray**: v2.29.1
- **Containerd**: v2.1.5
- **etcd**: v3.5.25
- **Python**: 3.12.9

**ë„¤íŠ¸ì›Œí¬ ì„¤ì •**:
- **Pod CIDR**: 10.233.64.0/18
- **Service CIDR**: 10.233.0.0/18
- **CNI**: Flannel
- **Kube Proxy Mode**: iptables

---

## ğŸ—ï¸ ì‹¤ìŠµ í™˜ê²½ êµ¬ì„±

### 1. ê°€ìƒë¨¸ì‹  êµ¬ì„±

**Vagrantë¥¼ ì´ìš©í•œ VM ìƒì„±**:

```bash
# ë””ë ‰í„°ë¦¬ ìƒì„± ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œ
mkdir k8s-ha-kubespary && cd k8s-ha-kubespary
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-kubespary/Vagrantfile
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-kubespary/admin-lb.sh
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-kubespary/init_cfg.sh

# VM ìƒì„± (ì•½ 5ë¶„ ì†Œìš”)
vagrant up

# ìƒíƒœ í™•ì¸
vagrant status

# admin-lb ì ‘ì†
vagrant ssh admin-lb
```

**Vagrantfile êµ¬ì¡°**:
```ruby
Vagrant.configure("2") do |config|
  config.vm.box = "rockylinux/10"

  # admin-lb ë…¸ë“œ
  config.vm.define "admin-lb" do |cfg|
    cfg.vm.hostname = "admin-lb"
    cfg.vm.network "private_network", ip: "192.168.10.10"
    cfg.vm.provision "shell", path: "admin-lb.sh"
  end

  # k8s-node1~5 ë…¸ë“œ
  (1..5).each do |i|
    config.vm.define "k8s-node#{i}" do |cfg|
      cfg.vm.hostname = "k8s-node#{i}"
      cfg.vm.network "private_network", ip: "192.168.10.1#{i}"
      cfg.vm.provision "shell", path: "init_cfg.sh"
    end
  end
end
```

---

### 2. ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„

#### (1) admin-lb.sh - Kubespray ì‹¤í–‰ ë…¸ë“œ

**ì£¼ìš” ì‘ì—…**:

```bash
#!/bin/bash

# 1. Timezone ë° NTP ì„¤ì •
timedatectl set-timezone Asia/Seoul
chronyc sources

# 2. Firewalld ë° SELinux ë¹„í™œì„±í™”
systemctl stop firewalld && systemctl disable firewalld
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# 3. HAProxy ì„¤ì¹˜ ë° ì„¤ì •
dnf install -y haproxy
cat <<EOF > /etc/haproxy/haproxy.cfg
global
    log /dev/log local0
    stats socket /var/lib/haproxy/stats
    stats timeout 30s

defaults
    log global
    mode tcp
    option tcplog
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

# Kubernetes API Server LoadBalancing
frontend kube-apiserver
    bind *:6443
    mode tcp
    default_backend kube-apiserver-backend

backend kube-apiserver-backend
    mode tcp
    balance roundrobin
    server k8s-node1 192.168.10.11:6443 check
    server k8s-node2 192.168.10.12:6443 check
    server k8s-node3 192.168.10.13:6443 check

# HAProxy í†µê³„ í˜ì´ì§€
frontend stats
    bind *:9000
    mode http
    stats enable
    stats uri /haproxy_stats
    stats refresh 30s

# Prometheus ë©”íŠ¸ë¦­
frontend prometheus
    bind *:8405
    http-request use-service prometheus-exporter if { path /metrics }
EOF

systemctl enable --now haproxy

# 4. NFS Server ì„¤ì¹˜
dnf install -y nfs-utils
mkdir -p /srv/nfs/share
chmod 777 /srv/nfs/share
echo "/srv/nfs/share *(rw,sync,no_root_squash)" >> /etc/exports
systemctl enable --now nfs-server

# 5. SSH í‚¤ ìƒì„± ë° ë°°í¬
ssh-keygen -t rsa -N "" -f /root/.ssh/id_rsa
for i in {1..5}; do
  sshpass -p 'qwe123' ssh-copy-id -o StrictHostKeyChecking=no root@192.168.10.1$i
done

# 6. Kubespray í´ë¡ 
cd /root
git clone -b v2.29.1 https://github.com/kubernetes-sigs/kubespray.git

# 7. kubectl, k9s, kubecolor, helm ì„¤ì¹˜
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl && mv kubectl /usr/local/bin/

curl -L https://github.com/derailed/k9s/releases/latest/download/k9s_linux_amd64.rpm -o k9s.rpm
dnf install -y k9s.rpm

curl -L https://github.com/hidetatz/kubecolor/releases/latest/download/kubecolor_*_linux_amd64.tar.gz | tar xz
mv kubecolor /usr/local/bin/

curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```

**HAProxy ì„¤ì • í¬ì¸íŠ¸**:
- âœ… **Frontend**: `*:6443` - ëª¨ë“  ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì‹ 
- âœ… **Backend**: roundrobin ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ 3ê°œ Control Planeì— ë¶„ì‚°
- âœ… **Health Check**: `check` ì˜µì…˜ìœ¼ë¡œ ì¥ì•  ë…¸ë“œ ìë™ ì œì™¸
- âœ… **í†µê³„ í˜ì´ì§€**: `http://192.168.10.10:9000/haproxy_stats`
- âœ… **Prometheus ë©”íŠ¸ë¦­**: `http://192.168.10.10:8405/metrics`

---

#### (2) init-cfg.sh - K8s ë…¸ë“œ ì´ˆê¸°í™”

**ì£¼ìš” ì‘ì—…**:

```bash
#!/bin/bash

# 1. Swap ë¹„í™œì„±í™” ë° íŒŒí‹°ì…˜ ì‚­ì œ
swapoff -a
sed -i '/swap/d' /etc/fstab

# 2. ì»¤ë„ ëª¨ë“ˆ ë¡œë“œ
cat <<EOF > /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter

# 3. ì»¤ë„ íŒŒë¼ë¯¸í„° ì„¤ì •
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF
sysctl --system

# 4. SSH ì„¤ì •
sed -i 's/^#PermitRootLogin yes/PermitRootLogin yes/' /etc/ssh/sshd_config
sed -i 's/^PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
systemctl restart sshd
```

**ì„¤ì • ì´ìœ **:
- âœ… **Swap ë¹„í™œì„±í™”**: KubernetesëŠ” ë©”ëª¨ë¦¬ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ì„ ìœ„í•´ Swap ì‚¬ìš© ê¸ˆì§€
- âœ… **overlay**: OverlayFS íŒŒì¼ì‹œìŠ¤í…œ (Container ì´ë¯¸ì§€ ë ˆì´ì–´ë§)
- âœ… **br_netfilter**: Bridge íŠ¸ë˜í”½ì´ iptablesë¥¼ ê±°ì¹˜ë„ë¡ ì„¤ì •
- âœ… **ip_forward**: Pod ê°„ í†µì‹ ì„ ìœ„í•œ IP í¬ì›Œë”©

---

### 3. Kubespray ë°°í¬

#### (1) Inventory ì„¤ì •

```bash
cd /root/kubespray/

# ê¸°ë³¸ inventory í™•ì¸
cat inventory/mycluster/inventory.ini
```

**inventory.ini êµ¬ì¡°**:
```ini
[all]
k8s-node1 ansible_host=192.168.10.11 ip=192.168.10.11
k8s-node2 ansible_host=192.168.10.12 ip=192.168.10.12
k8s-node3 ansible_host=192.168.10.13 ip=192.168.10.13
k8s-node4 ansible_host=192.168.10.14 ip=192.168.10.14
k8s-node5 ansible_host=192.168.10.15 ip=192.168.10.15

[kube_control_plane]
k8s-node1
k8s-node2
k8s-node3

[etcd]
k8s-node1
k8s-node2
k8s-node3

[kube_node]
k8s-node1
k8s-node2
k8s-node3
k8s-node4
k8s-node5

[k8s_cluster:children]
kube_control_plane
kube_node
```

**ì£¼ìš” ê·¸ë£¹**:
- âœ… **kube_control_plane**: API Server, Scheduler, Controller-Manager
- âœ… **etcd**: etcd í´ëŸ¬ìŠ¤í„° ë…¸ë“œ
- âœ… **kube_node**: kubelet, kube-proxy ì‹¤í–‰ ë…¸ë“œ (Control Planeë„ í¬í•¨)

---

#### (2) ë³€ìˆ˜ ì„¤ì •

```bash
# CNI ë³€ê²½: Calico â†’ Flannel
sed -i 's|kube_network_plugin: calico|kube_network_plugin: flannel|g' \
  inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

# Kube-Proxy ëª¨ë“œ ë³€ê²½: ipvs â†’ iptables
sed -i 's|kube_proxy_mode: ipvs|kube_proxy_mode: iptables|g' \
  inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

# NodeLocalDNS ë¹„í™œì„±í™”
sed -i 's|enable_nodelocaldns: true|enable_nodelocaldns: false|g' \
  inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

# DNS Autoscaler ë¹„í™œì„±í™”
echo "enable_dns_autoscaler: false" >> \
  inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

# Flannel ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
echo "flannel_interface: enp0s9" >> \
  inventory/mycluster/group_vars/k8s_cluster/k8s-net-flannel.yml

# Metrics Server í™œì„±í™”
sed -i 's|metrics_server_enabled: false|metrics_server_enabled: true|g' \
  inventory/mycluster/group_vars/k8s_cluster/addons.yml
```

**ì„¤ì • ì´ìœ **:
- âœ… **Flannel**: ê°€ë³ê³  ê°„ë‹¨í•œ CNI (VXLAN)
- âœ… **iptables**: ipvsë³´ë‹¤ ë””ë²„ê¹… ìš©ì´
- âœ… **flannel_interface**: Vagrant Private Network ì¸í„°í˜ì´ìŠ¤ ì§€ì •
- âœ… **NodeLocalDNS ë¹„í™œì„±í™”**: ì‹¤ìŠµ í™˜ê²½ ë‹¨ìˆœí™”

---

#### (3) ë°°í¬ ì‹¤í–‰

```bash
ANSIBLE_FORCE_COLOR=true ansible-playbook \
  -i inventory/mycluster/inventory.ini \
  -v cluster.yml \
  -e kube_version="1.32.9" \
  | tee kubespray_install.log
```

**ì†Œìš” ì‹œê°„**: ì•½ 8ë¶„

**ë°°í¬ ê³¼ì •**:
1. **Preflight Check**: ì‚¬ì „ ê²€ì¦
2. **Container Engine ì„¤ì¹˜**: Containerd, Runc, CNI Plugins
3. **etcd í´ëŸ¬ìŠ¤í„° êµ¬ì„±**: systemd unitìœ¼ë¡œ 3-Node í´ëŸ¬ìŠ¤í„°
4. **Kubernetes ì„¤ì¹˜**: kubeadm, kubelet, kubectl
5. **Control Plane êµ¬ì„±**: kubeadm init (ì²« ë²ˆì§¸ ë…¸ë“œ) â†’ join (ë‚˜ë¨¸ì§€ ë…¸ë“œ)
6. **CNI ì„¤ì¹˜**: Flannel DaemonSet
7. **ì• ë“œì˜¨ ì„¤ì¹˜**: CoreDNS, Metrics Server

---

#### (4) ë°°í¬ í™•ì¸

```bash
# kubeconfig ë³µì‚¬
mkdir /root/.kube
scp k8s-node1:/root/.kube/config /root/.kube/
sed -i 's/127.0.0.1/192.168.10.11/g' /root/.kube/config

# ë…¸ë“œ í™•ì¸
kubectl get node -owide
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
NAME         STATUS   ROLES           AGE   VERSION   INTERNAL-IP     OS-IMAGE
k8s-node1    Ready    control-plane   5m    v1.32.9   192.168.10.11   Rocky Linux 10.0
k8s-node2    Ready    control-plane   5m    v1.32.9   192.168.10.12   Rocky Linux 10.0
k8s-node3    Ready    control-plane   5m    v1.32.9   192.168.10.13   Rocky Linux 10.0
k8s-node4    Ready    <none>          5m    v1.32.9   192.168.10.14   Rocky Linux 10.0
k8s-node5    Ready    <none>          5m    v1.32.9   192.168.10.15   Rocky Linux 10.0
```

```bash
# etcd í´ëŸ¬ìŠ¤í„° í™•ì¸
ssh k8s-node1 etcdctl.sh member list -w table
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
+------------------+---------+-----------+----------------------------+----------------------------+
|        ID        | STATUS  |   NAME    |         PEER ADDRS         |        CLIENT ADDRS        |
+------------------+---------+-----------+----------------------------+----------------------------+
| 8e9e05c52164694d | started | k8s-node1 | https://192.168.10.11:2380 | https://192.168.10.11:2379 |
| 2f98a53f33e3d3a4 | started | k8s-node2 | https://192.168.10.12:2380 | https://192.168.10.12:2379 |
| fd422379fda50e48 | started | k8s-node3 | https://192.168.10.13:2380 | https://192.168.10.13:2379 |
+------------------+---------+-----------+----------------------------+----------------------------+
```

```bash
# Control Plane Static Pod í™•ì¸
kubectl get pod -n kube-system -o wide | grep -E "node1|node2|node3"
```

**ì¶œë ¥**:
```
kube-apiserver-k8s-node1            1/1   Running   k8s-node1
kube-controller-manager-k8s-node1   1/1   Running   k8s-node1
kube-scheduler-k8s-node1            1/1   Running   k8s-node1
kube-apiserver-k8s-node2            1/1   Running   k8s-node2
kube-controller-manager-k8s-node2   1/1   Running   k8s-node2
kube-scheduler-k8s-node3            1/1   Running   k8s-node3
```

---

## ğŸŒ K8S API ì—”ë“œí¬ì¸íŠ¸ ì „ëµ

Kubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ **API Serverì— ì ‘ê·¼í•˜ëŠ” ë°©ë²•**ì€ ë‹¤ìŒ 3ê°€ì§€ Caseë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.

---

### Case 1: Client-Side LoadBalancing

**êµ¬ì¡°**: HA Control Plane + Worker Client-Side LoadBalancing

```mermaid
graph TB
    subgraph "Control Plane Nodes"
        CP1["k8s-node1<br/>API Server: 127.0.0.1:6443"]
        CP2["k8s-node2<br/>API Server: 127.0.0.1:6443"]
        CP3["k8s-node3<br/>API Server: 127.0.0.1:6443"]
    end

    subgraph "Worker Nodes"
        W1["k8s-node4"]
        W2["k8s-node5"]

        subgraph "Nginx Static Pod (k8s-node4)"
            N1["Nginx LB<br/>127.0.0.1:6443"]
        end

        subgraph "Nginx Static Pod (k8s-node5)"
            N2["Nginx LB<br/>127.0.0.1:6443"]
        end
    end

    W1 --> N1
    W2 --> N2

    N1 -->|least_conn| CP1
    N1 -->|least_conn| CP2
    N1 -->|least_conn| CP3

    N2 -->|least_conn| CP1
    N2 -->|least_conn| CP2
    N2 -->|least_conn| CP3

    CP1 -->|ìê¸° ìì‹ | CP1
    CP2 -->|ìê¸° ìì‹ | CP2
    CP3 -->|ìê¸° ìì‹ | CP3
```

**íŠ¹ì§•**:
- âœ… **Control Plane ë…¸ë“œ**: ë¡œì»¬ API Server ì§ì ‘ ì ‘ê·¼ (`127.0.0.1:6443`)
- âœ… **Worker ë…¸ë“œ**: Nginx Static Podë¥¼ í†µí•œ Client-Side LB
- âœ… **ì™¸ë¶€ LB ë¶ˆí•„ìš”**: ê° ë…¸ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ LB ìš´ì˜
- âœ… **ì¥ì•  ì¡°ì¹˜**: Nginxê°€ `least_conn` ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìë™ failover

**Nginx ì„¤ì •** (`/etc/kubernetes/nginx-proxy.conf`):

```nginx
error_log stderr notice;

events {
    worker_connections 1024;
}

stream {
    upstream kube_apiserver {
        least_conn;
        server 192.168.10.11:6443;
        server 192.168.10.12:6443;
        server 192.168.10.13:6443;
    }

    server {
        listen        127.0.0.1:6443;
        proxy_pass    kube_apiserver;
        proxy_timeout 10m;
        proxy_connect_timeout 1s;
    }
}
```

**Nginx Static Pod Manifest** (`/etc/kubernetes/manifests/nginx-proxy.yaml`):

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-proxy
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - name: nginx-proxy
    image: docker.io/library/nginx:1.27.3-alpine
    command:
    - nginx
    - -c
    - /etc/nginx/nginx.conf
    - -g
    - daemon off;
    volumeMounts:
    - name: nginx-config
      mountPath: /etc/nginx/nginx.conf
      readOnly: true
  volumes:
  - name: nginx-config
    hostPath:
      path: /etc/kubernetes/nginx-proxy.conf
```

**kubelet ì—”ë“œí¬ì¸íŠ¸ í™•ì¸**:

```bash
# Worker ë…¸ë“œì—ì„œ kubeletì´ ì‚¬ìš©í•˜ëŠ” API Server ì—”ë“œí¬ì¸íŠ¸
ssh k8s-node4 grep "server:" /etc/kubernetes/kubelet.conf
```

**ì¶œë ¥**:
```yaml
server: https://127.0.0.1:6443
```

---

### Case 2: External LB + Client-Side LB

**êµ¬ì¡°**: External LB (HAProxy) + HA Control Plane + Worker Client-Side LB

```mermaid
graph TB
    subgraph "admin-lb"
        HAP["HAProxy<br/>192.168.10.10:6443"]
    end

    subgraph "Control Plane Nodes"
        CP1["k8s-node1<br/>API Server"]
        CP2["k8s-node2<br/>API Server"]
        CP3["k8s-node3<br/>API Server"]
    end

    subgraph "Worker Nodes"
        W1["k8s-node4<br/>Nginx LB"]
        W2["k8s-node5<br/>Nginx LB"]
    end

    HAP -->|roundrobin| CP1
    HAP -->|roundrobin| CP2
    HAP -->|roundrobin| CP3

    W1 -->|least_conn| CP1
    W1 -->|least_conn| CP2
    W1 -->|least_conn| CP3

    W2 -->|least_conn| CP1
    W2 -->|least_conn| CP2
    W2 -->|least_conn| CP3

    CP1 -->|127.0.0.1:6443| CP1
    CP2 -->|127.0.0.1:6443| CP2
    CP3 -->|127.0.0.1:6443| CP3
```

**íŠ¹ì§•**:
- âœ… **External LB**: kubectl ë“± ì™¸ë¶€ ì ‘ê·¼ìš©
- âœ… **Control Plane**: ë¡œì»¬ API Server ì§ì ‘ ì ‘ê·¼
- âœ… **Worker**: Client-Side LB (Nginx Static Pod)
- âœ… **ì´ì¤‘ ì¥ì•  ë³´í˜¸**: HAProxy + Nginx ëª¨ë‘ ì¥ì•  ëŒ€ì‘

**ì„¤ì • ë°©ë²•**:

```yaml
# inventory/mycluster/group_vars/all/all.yml
apiserver_loadbalancer_domain_name: "k8s-api-srv.admin-lb.com"
loadbalancer_apiserver:
  address: 192.168.10.10
  port: 6443

# ì¸ì¦ì„œ SANì— External LB IP ì¶”ê°€
supplementary_addresses_in_ssl_keys: [192.168.10.10, k8s-api-srv.admin-lb.com]
```

**ì¬ë°°í¬** (Control Plane ì¸ì¦ì„œë§Œ):

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v cluster.yml \
  --tags "control-plane" \
  --limit kube_control_plane \
  -e kube_version="1.32.9"
```

**ì¸ì¦ì„œ SAN í™•ì¸**:

```bash
ssh k8s-node1 openssl x509 -in /etc/kubernetes/ssl/apiserver.crt -text -noout | grep -A1 "Subject Alternative Name"
```

**ì¶œë ¥**:
```
X509v3 Subject Alternative Name:
    DNS:k8s-api-srv.admin-lb.com, DNS:k8s-node1, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:192.168.10.10, IP Address:192.168.10.11, IP Address:192.168.10.12, IP Address:192.168.10.13, IP Address:10.233.0.1
```

---

### Case 3: External LB Only

**êµ¬ì¡°**: ëª¨ë“  ë…¸ë“œê°€ External LB ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©

```mermaid
graph TB
    subgraph "admin-lb"
        HAP["HAProxy<br/>192.168.10.10:6443"]
    end

    subgraph "Control Plane Nodes"
        CP1["k8s-node1"]
        CP2["k8s-node2"]
        CP3["k8s-node3"]
    end

    subgraph "Worker Nodes"
        W1["k8s-node4"]
        W2["k8s-node5"]
    end

    CP1 -->|192.168.10.10:6443| HAP
    CP2 -->|192.168.10.10:6443| HAP
    CP3 -->|192.168.10.10:6443| HAP
    W1 -->|192.168.10.10:6443| HAP
    W2 -->|192.168.10.10:6443| HAP

    HAP -->|roundrobin| CP1
    HAP -->|roundrobin| CP2
    HAP -->|roundrobin| CP3
```

**íŠ¹ì§•**:
- âœ… **ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸**: ëª¨ë“  ë…¸ë“œê°€ HAProxy ì‚¬ìš©
- âœ… **Client-Side LB ë¹„í™œì„±í™”**: Nginx Static Pod ë¯¸ë°°í¬
- âœ… **ì¤‘ì•™ ì§‘ì¤‘ì‹ ê´€ë¦¬**: LB ì„¤ì • ë³€ê²½ ìš©ì´
- âš ï¸ **SPOF**: HAProxy ìì²´ê°€ ë‹¨ì¼ ì¥ì• ì 

**ì„¤ì • ë°©ë²•**:

```yaml
# inventory/mycluster/group_vars/all/all.yml
apiserver_loadbalancer_domain_name: "k8s-api-srv.admin-lb.com"
loadbalancer_apiserver:
  address: 192.168.10.10
  port: 6443
loadbalancer_apiserver_localhost: false  # Client-Side LB ë¯¸ì‚¬ìš©
supplementary_addresses_in_ssl_keys: [192.168.10.10, k8s-api-srv.admin-lb.com]
```

**í”„ë¡œë•ì…˜ í™˜ê²½**:
- âœ… **HAProxy HA**: Keepalived + VIPë¡œ HAProxy ìì²´ HA êµ¬ì„±
- âœ… **Cloud LB**: AWS ELB, GCP LB, Azure LB ì‚¬ìš©

---

## ğŸ”§ ë…¸ë“œ ê´€ë¦¬

### 1. ë…¸ë“œ ì¶”ê°€ (scale.yml)

**ì‹œë‚˜ë¦¬ì˜¤**: k8s-node5 Worker ë…¸ë“œë¥¼ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€

#### (1) scale.yml ì‹¤í–‰ íë¦„

```mermaid
graph TB
    Start["scale.yml ì‹¤í–‰"] --> Check["1. Preflight Check<br/>(ë…¸ë“œ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€)"]
    Check --> etcd["2. etcd ì„¤ì •<br/>(Control Plane ë…¸ë“œë§Œ)"]
    etcd --> ContainerEngine["3. Container Engine ì„¤ì¹˜<br/>(Containerd, Runc, CNI Plugins)"]
    ContainerEngine --> K8sInstall["4. Kubernetes ì„¤ì¹˜<br/>(kubeadm, kubelet, kubectl)"]
    K8sInstall --> CertUpload["5. Control Plane ì¸ì¦ì„œ ì—…ë¡œë“œ<br/>(certificate-key ì¶”ì¶œ)"]
    CertUpload --> JoinDecision{"ë…¸ë“œ ì—­í• ?"}

    JoinDecision -->|Control Plane| CPJoin["6a. kubeadm join --control-plane"]
    JoinDecision -->|Worker| WorkerJoin["6b. kubeadm join"]

    CPJoin --> CNI["7. CNI ì„¤ì • ì ìš©"]
    WorkerJoin --> CNI

    CNI --> Label["8. Node Label ë° Taint ì ìš©"]
    Label --> DNS["9. DNS ì„¤ì • ì—…ë°ì´íŠ¸"]
    DNS --> End["ë…¸ë“œ ì¶”ê°€ ì™„ë£Œ"]
```

---

#### (2) ì‹¤ìŠµ: k8s-node5 ì œê±° í›„ ì¬ì¶”ê°€

**Step 1**: k8s-node5 ì œê±° (í…ŒìŠ¤íŠ¸ìš©)

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v remove-node.yml \
  -e node=k8s-node5
```

**Step 2**: inventory.ini ìˆ˜ì •

```bash
vi inventory/mycluster/inventory.ini
```

```ini
# k8s-node5ë¥¼ [kube_node] ê·¸ë£¹ì— ì¶”ê°€
[kube_node]
k8s-node1
k8s-node2
k8s-node3
k8s-node4
k8s-node5  # ì¶”ê°€
```

**Step 3**: scale.yml ì‹¤í–‰

```bash
ANSIBLE_FORCE_COLOR=true ansible-playbook \
  -i inventory/mycluster/inventory.ini \
  -v scale.yml \
  --limit=k8s-node5 \
  -e kube_version="1.32.9"
```

**ì†Œìš” ì‹œê°„**: ì•½ 3ë¶„

**Step 4**: ë…¸ë“œ í™•ì¸

```bash
kubectl get node
```

**ì¶œë ¥**:
```
NAME         STATUS   ROLES           AGE
k8s-node1    Ready    control-plane   20m
k8s-node2    Ready    control-plane   20m
k8s-node3    Ready    control-plane   20m
k8s-node4    Ready    <none>          20m
k8s-node5    Ready    <none>          1m   # ìƒˆë¡œ ì¶”ê°€ë¨
```

---

#### (3) scale.yml ì£¼ìš” Task ë¶„ì„

**Task 1**: etcd ì„¤ì • (Control Plane ë…¸ë“œë§Œ)

```yaml
- name: Join etcd member
  command: |
    etcdctl member add k8s-node5 \
      --peer-urls=https://192.168.10.15:2380
  when: "'kube_control_plane' in group_names"
```

**Task 2**: Container Engine ì„¤ì¹˜

```yaml
- name: Install containerd
  import_role:
    name: container-engine/containerd
```

**Task 3**: kubeadm join ì‹¤í–‰

```yaml
- name: Join node to cluster
  command: |
    kubeadm join 192.168.10.11:6443 \
      --token {{ bootstrap_token }} \
      --discovery-token-ca-cert-hash sha256:{{ ca_cert_hash }} \
      --node-name k8s-node5
```

**Task 4**: Node Label ì ìš©

```yaml
- name: Label node as worker
  command: |
    kubectl label node k8s-node5 \
      node-role.kubernetes.io/worker=worker
  delegate_to: "{{ groups['kube_control_plane'][0] }}"
```

---

### 2. ë…¸ë“œ ì œê±° (remove-node.yml)

**ì‹œë‚˜ë¦¬ì˜¤**: k8s-node5 Worker ë…¸ë“œë¥¼ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì œê±°

#### (1) remove-node.yml ì‹¤í–‰ íë¦„

```mermaid
graph TB
    Start["remove-node.yml ì‹¤í–‰"] --> Confirm["1. ì‚¬ìš©ì í™•ì¸<br/>(yes ì…ë ¥ í•„ìš”)"]
    Confirm --> Drain["2. Node Drain<br/>(Pod ì¶•ì¶œ)"]
    Drain --> PDBCheck{"PDB ì„¤ì •?"}

    PDBCheck -->|maxUnavailable: 0| Timeout["Drain íƒ€ì„ì•„ì›ƒ<br/>(ì¶•ì¶œ ë¶ˆê°€)"]
    PDBCheck -->|ì •ìƒ| DrainSuccess["Drain ì„±ê³µ"]

    Timeout --> Manual["ìˆ˜ë™ PDB ì‚­ì œ í•„ìš”"]
    DrainSuccess --> Kubelet["3. Kubelet ì¤‘ì§€"]

    Kubelet --> etcdCheck{"etcd ë…¸ë“œ?"}
    etcdCheck -->|Yes| etcdRemove["4a. etcd ë©¤ë²„ ì œê±°"]
    etcdCheck -->|No| Reset["4b. Kubeadm Reset"]

    etcdRemove --> Reset
    Reset --> Delete["5. kubectl delete node"]
    Delete --> End["ë…¸ë“œ ì œê±° ì™„ë£Œ"]
```

---

#### (2) ì‹¤ìŠµ: k8s-node5 ì œê±°

**Step 1**: remove-node.yml ì‹¤í–‰

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v remove-node.yml \
  -e node=k8s-node5
```

**í”„ë¡¬í”„íŠ¸**:
```
Are you sure you want to remove node k8s-node5? (yes/no)
> yes
```

**ì†Œìš” ì‹œê°„**: ì•½ 2ë¶„ (PDB ì—†ì„ ê²½ìš° 20ì´ˆ)

**Step 2**: ë…¸ë“œ í™•ì¸

```bash
kubectl get node
```

**ì¶œë ¥**:
```
NAME         STATUS   ROLES           AGE
k8s-node1    Ready    control-plane   25m
k8s-node2    Ready    control-plane   25m
k8s-node3    Ready    control-plane   25m
k8s-node4    Ready    <none>          25m
# k8s-node5 ì œê±°ë¨
```

---

#### (3) PodDisruptionBudget (PDB) ì´ìŠˆ

**ë¬¸ì œ**: `maxUnavailable: 0` ì„¤ì •ìœ¼ë¡œ Drain íƒ€ì„ì•„ì›ƒ

**PDB ì˜ˆì‹œ**:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webpod
spec:
  maxUnavailable: 0  # Drain ì‹œ ì¶•ì¶œ ë¶ˆê°€
  selector:
    matchLabels:
      app: webpod
```

**Drain ì‹¤íŒ¨ ë¡œê·¸**:

```
TASK [remove-node/pre-remove : Drain node] ****
fatal: [k8s-node5]: FAILED! => {
  "msg": "error: cannot delete Pods with local storage (use --delete-emptydir-data to override): default/webpod-xxx\nerror: unable to drain node \"k8s-node5\" due to error:cannot delete Pods with local storage (continuing command...)\nThere are pending nodes to be drained:\n k8s-node5\nerror: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-xxx"
}
```

**í•´ê²° ë°©ë²•**:

1. **PDB ì‚­ì œ**:
```bash
kubectl delete pdb webpod
```

2. **PDB ì„¤ì • ë³€ê²½**:
```bash
kubectl edit pdb webpod
# maxUnavailable: 0 â†’ 1ë¡œ ë³€ê²½
```

3. **ê°•ì œ Drain**:
```bash
kubectl drain k8s-node5 \
  --ignore-daemonsets \
  --delete-emptydir-data \
  --force
```

---

### 3. ë¹„ì •ìƒ ë…¸ë“œ ê°•ì œ ì‚­ì œ

**ì‹œë‚˜ë¦¬ì˜¤**: NotReady ìƒíƒœì´ë©° SSH ì ‘ì† ë¶ˆê°€ëŠ¥í•œ ë…¸ë“œ ì œê±°

#### (1) ë¬¸ì œ ìƒí™©

```bash
# k8s-node5ê°€ NotReady ìƒíƒœ
kubectl get node
```

**ì¶œë ¥**:
```
NAME         STATUS     ROLES   AGE
k8s-node5    NotReady   <none>  10m
```

**SSH ì ‘ì† ì‹¤íŒ¨**:
```bash
ssh k8s-node5
# Connection refused
```

---

#### (2) ê°•ì œ ì‚­ì œ ëª…ë ¹ì–´

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v remove-node.yml \
  -e node=k8s-node5 \
  -e reset_nodes=false \
  -e allow_ungraceful_removal=true \
  -e skip_confirmation=true
```

**íŒŒë¼ë¯¸í„° ì„¤ëª…**:
- âœ… **reset_nodes=false**: ë…¸ë“œì— SSH ì ‘ì†í•˜ì—¬ kubeadm reset ìˆ˜í–‰ ì•ˆ í•¨
- âœ… **allow_ungraceful_removal=true**: Drain ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
- âœ… **skip_confirmation=true**: í™•ì¸ í”„ë¡¬í”„íŠ¸ ìŠ¤í‚µ

---

#### (3) ìˆ˜ë™ ì •ë¦¬ (í•„ìš” ì‹œ)

```bash
# etcd ë©¤ë²„ ì œê±° (Control Plane ë…¸ë“œì˜€ì„ ê²½ìš°)
ssh k8s-node1 etcdctl.sh member list
ssh k8s-node1 etcdctl.sh member remove <member-id>

# Kubernetes ë…¸ë“œ ì‚­ì œ
kubectl delete node k8s-node5

# iptables ê·œì¹™ ì •ë¦¬
ssh k8s-node1 iptables -t nat -D KUBE-SERVICES -d 192.168.10.15 -j REJECT
```

---

### 4. í´ëŸ¬ìŠ¤í„° ë¦¬ì…‹ (reset.yml)

**ê²½ê³ **: âš ï¸ **ì „ì²´ í´ëŸ¬ìŠ¤í„°ë¥¼ ì„¤ì¹˜ ì „ ìƒíƒœë¡œ ì™„ì „ ì œê±° (ë³µêµ¬ ë¶ˆê°€)**

#### (1) reset.yml ì‹¤í–‰

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v reset.yml
```

**í”„ë¡¬í”„íŠ¸**:
```
Are you sure you want to reset the entire cluster? (yes/no)
> yes
```

---

#### (2) reset.yml ì£¼ìš” Task

```yaml
# 1. kubectl drain (ëª¨ë“  ë…¸ë“œ)
- name: Drain all nodes
  command: kubectl drain {{ inventory_hostname }} --ignore-daemonsets --delete-emptydir-data

# 2. kubeadm reset (ëª¨ë“  ë…¸ë“œ)
- name: Reset kubeadm
  command: kubeadm reset -f

# 3. etcd ë°ì´í„° ì‚­ì œ
- name: Remove etcd data
  file:
    path: /var/lib/etcd
    state: absent

# 4. Kubernetes ì„¤ì • ì‚­ì œ
- name: Remove Kubernetes config
  file:
    path: "{{ item }}"
    state: absent
  loop:
  - /etc/kubernetes
  - /var/lib/kubelet
  - /var/lib/dockershim
  - /var/run/kubernetes
  - /var/lib/cni
  - /etc/cni/net.d

# 5. Container Runtime ì¤‘ì§€
- name: Stop containerd
  systemd:
    name: containerd
    state: stopped

# 6. iptables ê·œì¹™ ì‚­ì œ
- name: Flush iptables
  command: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
```

---

#### (3) ì¬ë°°í¬

```bash
# ë‹¤ì‹œ cluster.yml ì‹¤í–‰
ANSIBLE_FORCE_COLOR=true ansible-playbook \
  -i inventory/mycluster/inventory.ini \
  -v cluster.yml \
  -e kube_version="1.32.9"
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •

### 1. kube-ops-view ì„¤ì¹˜

**kube-ops-view**: Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” ë„êµ¬

#### (1) Helm ì„¤ì¹˜

```bash
helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
helm repo update

helm install kube-ops-view geek-cookbook/kube-ops-view \
  --version 1.2.2 \
  --set service.main.type=NodePort \
  --set service.main.ports.http.nodePort=30000 \
  --set env.TZ="Asia/Seoul" \
  --namespace kube-system \
  --set image.repository="abihf/kube-ops-view" \
  --set image.tag="latest"
```

---

#### (2) ì ‘ì†

```bash
# ë¸Œë¼ìš°ì € ì—´ê¸°
open "http://192.168.10.14:30000/#scale=1.5"
```

**í™”ë©´ êµ¬ì„±**:
- âœ… **ë…¸ë“œë³„ Pod ë°°ì¹˜**: ê° ë…¸ë“œì— ì‹¤í–‰ ì¤‘ì¸ Pod ì‹œê°í™”
- âœ… **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ **: CPU, Memory ì‚¬ìš©ë¥  í‘œì‹œ
- âœ… **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**: Pod ìƒì„±/ì‚­ì œ ì‹¤ì‹œê°„ ë°˜ì˜

---

#### (3) ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë‹ˆí„°ë§

**ì‹ ê·œ í„°ë¯¸ë„ 1**: kube-ops-view ì ‘ì†
```bash
open "http://192.168.10.14:30000/#scale=1.5"
```

**ì‹ ê·œ í„°ë¯¸ë„ 2**: ë…¸ë“œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
```bash
watch -d kubectl get node
```

**admin-lb**: k8s-node1 ì¤‘ì§€
```bash
ssh k8s-node1 poweroff
```

**ê²°ê³¼**:
- âœ… k8s-node1 ìƒíƒœ: Ready â†’ NotReady (ì•½ 40ì´ˆ í›„)
- âœ… k8s-node1 Pod ìƒíƒœ: Terminating â†’ ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì¬ìƒì„±
- âœ… API Server ì ‘ê·¼: ì •ìƒ (k8s-node2, k8s-node3ë¡œ ìë™ failover)

---

### 2. HAProxy í†µê³„ í˜ì´ì§€

**ì ‘ì†**:
```bash
open "http://192.168.10.10:9000/haproxy_stats"
```

**ì£¼ìš” ì§€í‘œ**:
- âœ… **Backend Status**: k8s-node1~3ì˜ Health Check ìƒíƒœ
- âœ… **Session Rate**: ì´ˆë‹¹ ìš”ì²­ ìˆ˜
- âœ… **Total Sessions**: ì´ ì„¸ì…˜ ìˆ˜
- âœ… **Bytes In/Out**: íŠ¸ë˜í”½ í†µê³„

**Prometheus ë©”íŠ¸ë¦­**:
```bash
curl http://192.168.10.10:8405/metrics
```

**ì£¼ìš” ë©”íŠ¸ë¦­**:
```
haproxy_backend_up{backend="kube-apiserver-backend"} 3
haproxy_server_up{server="k8s-node1"} 1
haproxy_server_up{server="k8s-node2"} 1
haproxy_server_up{server="k8s-node3"} 1
```

---

## ğŸ’¡ í•µì‹¬ ê°œë… ì •ë¦¬

### 1. Client-Side vs External LoadBalancing

#### (1) ë¹„êµí‘œ

| í•­ëª© | Client-Side LB | External LB |
|------|----------------|-------------|
| **êµ¬í˜„** | Nginx Static Pod (ê° ë…¸ë“œ) | HAProxy, AWS ELB, GCP LB |
| **ì¥ì ** | ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ, ìë™ failover | ì¤‘ì•™ ì§‘ì¤‘ì‹ ê´€ë¦¬, ëª¨ë‹ˆí„°ë§ ìš©ì´ |
| **ë‹¨ì ** | ë¦¬ì†ŒìŠ¤ ì˜¤ë²„í—¤ë“œ, ì„¤ì • ë³µì¡ | SPOF, ì¸í”„ë¼íŒ€ í˜‘ì—… í•„ìš” |
| **ì í•© í™˜ê²½** | On-Premise, ì†Œê·œëª¨ í´ëŸ¬ìŠ¤í„° | Cloud, ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ |

---

#### (2) Client-Side LoadBalancing ë™ì‘ ì›ë¦¬

```mermaid
graph TB
    subgraph "k8s-node4 (Worker)"
        Kubelet["kubelet"]
        NginxPod["Nginx Static Pod<br/>(hostNetwork: true)"]

        Kubelet -->|API ìš”ì²­| Local["127.0.0.1:6443"]
        Local --> NginxPod
    end

    subgraph "Control Plane Nodes"
        CP1["k8s-node1:6443"]
        CP2["k8s-node2:6443"]
        CP3["k8s-node3:6443"]
    end

    NginxPod -->|least_conn| CP1
    NginxPod -->|least_conn| CP2
    NginxPod -->|least_conn| CP3

    CP1 -.->|Health Check| NginxPod
    CP2 -.->|Health Check| NginxPod
    CP3 -.->|Health Check| NginxPod
```

**ë™ì‘ ê³¼ì •**:
1. kubeletì´ `127.0.0.1:6443`ìœ¼ë¡œ API ìš”ì²­
2. Nginx Static Podê°€ ìš”ì²­ ìˆ˜ì‹ 
3. `least_conn` ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê°€ì¥ ì ì€ ì—°ê²° ìˆ˜ë¥¼ ê°€ì§„ API Server ì„ íƒ
4. ì„ íƒëœ API Serverë¡œ ìš”ì²­ ì „ë‹¬
5. Health Check ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ë…¸ë“œ ìë™ ì œì™¸

---

#### (3) External LoadBalancing (HAProxy)

```mermaid
graph TB
    subgraph "admin-lb"
        Frontend["HAProxy Frontend<br/>*:6443"]
        Backend["HAProxy Backend<br/>(roundrobin)"]
        Stats["í†µê³„ í˜ì´ì§€<br/>:9000"]
        Prometheus["Prometheus ë©”íŠ¸ë¦­<br/>:8405"]

        Frontend --> Backend
        Backend -.-> Stats
        Backend -.-> Prometheus
    end

    subgraph "Control Plane Nodes"
        CP1["k8s-node1:6443"]
        CP2["k8s-node2:6443"]
        CP3["k8s-node3:6443"]
    end

    subgraph "Client"
        kubectl["kubectl<br/>(External)"]
        kubelet["kubelet<br/>(Worker)"]
    end

    kubectl -->|192.168.10.10:6443| Frontend
    kubelet -->|192.168.10.10:6443| Frontend

    Backend -->|roundrobin| CP1
    Backend -->|roundrobin| CP2
    Backend -->|roundrobin| CP3

    CP1 -.->|Health Check| Backend
    CP2 -.->|Health Check| Backend
    CP3 -.->|Health Check| Backend
```

**ë™ì‘ ê³¼ì •**:
1. Clientê°€ `192.168.10.10:6443`ìœ¼ë¡œ ìš”ì²­
2. HAProxy Frontendê°€ ìš”ì²­ ìˆ˜ì‹ 
3. `roundrobin` ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ API Server ì„ íƒ
4. Health Check ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ë…¸ë“œ ìë™ ì œì™¸
5. í†µê³„ í˜ì´ì§€ ë° Prometheus ë©”íŠ¸ë¦­ ì œê³µ

---

### 2. Kubespray ë³€ìˆ˜ ìš°ì„ ìˆœìœ„

```mermaid
graph TB
    Start["ë³€ìˆ˜ ê²€ìƒ‰ ì‹œì‘"] --> RoleDefaults["1. roles/*/defaults/main.yml<br/>(ê¸°ë³¸ê°’)"]
    RoleDefaults -->|override| RoleVars["2. roles/*/vars/main.yml<br/>(ë‚´ë¶€ ê°•ì œ ë³€ìˆ˜)"]
    RoleVars -->|override| GroupVars["3. inventory/mycluster/group_vars/*.yml<br/>(ì „ì²´ ë…¸ë“œ ê³µí†µ ì„¤ì •)"]
    GroupVars -->|override| HostVars["4. inventory/mycluster/host_vars/<node>.yml<br/>(íŠ¹ì • ë…¸ë“œë§Œ)"]
    HostVars -->|override| PlaybookVars["5. playbook vars (vars:, vars_files:)<br/>(í”Œë ˆì´ë¶ ë¡œì»¬ ë³€ìˆ˜)"]
    PlaybookVars -->|override| ExtraVars["6. --extra-vars (-e)<br/>(CLIì—ì„œ ì¤€ ê°’ - ìµœê°•ì)"]
    ExtraVars --> End["ìµœì¢… ë³€ìˆ˜ ê°’ ê²°ì •"]

    style ExtraVars fill:#ff6b6b
    style GroupVars fill:#51cf66
```

**ì‹¤ì „ íŒ**:
- âœ… **99% ì¡°ì ˆ**: `inventory/mycluster/group_vars/`ì—ì„œ ì¡°ì ˆ
- âœ… **ê¸´ê¸‰ ë³€ê²½**: `-e` ì˜µì…˜ìœ¼ë¡œ CLIì—ì„œ ì¦‰ì‹œ override
- âœ… **íŠ¹ì • ë…¸ë“œë§Œ**: `host_vars/` ì‚¬ìš©
- âœ… **ë³€ìˆ˜ ê²€ìƒ‰**: `grep -Rn "ë³€ìˆ˜ëª…" inventory/ roles/ playbooks/`

**ì˜ˆì‹œ**:

```bash
# kube_version ë³€ìˆ˜ ê²€ìƒ‰
grep -Rn "kube_version" inventory/mycluster/ roles/ playbooks/

# ìš°ì„ ìˆœìœ„ í™•ì¸
# 1. roles/kubernetes/defaults/main.yml: kube_version: "1.32.0" (ê¸°ë³¸ê°’)
# 2. inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml: kube_version: "1.32.9" (ì‚¬ìš©ì ì„¤ì •)
# 3. CLI: -e kube_version="1.33.0" (ìµœìš°ì„ )
```

---

### 3. etcd Deployment Type

#### (1) host (systemd unit) - Kubespray ê¸°ë³¸ê°’

```yaml
# inventory/mycluster/group_vars/etcd.yml
etcd_deployment_type: host
```

**íŠ¹ì§•**:
- âœ… **ë…ë¦½ ê´€ë¦¬**: systemd unitìœ¼ë¡œ etcd ì‹¤í–‰
- âœ… **kubeadm ë…ë¦½**: kubeadmì— ì¢…ì†ë˜ì§€ ì•ŠìŒ
- âœ… **ì—…ê·¸ë ˆì´ë“œ ìš©ì´**: etcdë§Œ ë³„ë„ ì—…ê·¸ë ˆì´ë“œ ê°€ëŠ¥
- âœ… **ë°±ì—…/ë³µêµ¬**: systemctlë¡œ ê°„í¸í•˜ê²Œ ê´€ë¦¬

**etcd.service**:

```ini
[Unit]
Description=etcd
After=network.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name=k8s-node1 \
  --data-dir=/var/lib/etcd \
  --listen-client-urls=https://192.168.10.11:2379 \
  --advertise-client-urls=https://192.168.10.11:2379 \
  --listen-peer-urls=https://192.168.10.11:2380 \
  --initial-advertise-peer-urls=https://192.168.10.11:2380 \
  --cert-file=/etc/ssl/etcd/ssl/member-k8s-node1.pem \
  --key-file=/etc/ssl/etcd/ssl/member-k8s-node1-key.pem \
  --peer-cert-file=/etc/ssl/etcd/ssl/member-k8s-node1.pem \
  --peer-key-file=/etc/ssl/etcd/ssl/member-k8s-node1-key.pem \
  --trusted-ca-file=/etc/ssl/etcd/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/ssl/etcd/ssl/ca.pem
Restart=on-failure

[Install]
WantedBy=multi-user.target
```

**ê´€ë¦¬ ëª…ë ¹ì–´**:

```bash
# etcd ìƒíƒœ í™•ì¸
systemctl status etcd

# etcd ì¬ì‹œì‘
systemctl restart etcd

# etcd ë¡œê·¸ í™•ì¸
journalctl -u etcd -f

# etcd ë©¤ë²„ í™•ì¸
etcdctl member list -w table

# etcd ë°±ì—…
etcdctl snapshot save /tmp/etcd-backup.db
```

---

#### (2) kubeadm (Static Pod)

```yaml
# inventory/mycluster/group_vars/etcd.yml
etcd_deployment_type: kubeadm
```

**íŠ¹ì§•**:
- âœ… **kubeadm í†µí•©**: kubeadmì´ etcd ê´€ë¦¬
- âœ… **Static Pod**: `/etc/kubernetes/manifests/etcd.yaml`
- âš ï¸ **ì¢…ì†ì„±**: kubeadmì— ì˜ì¡´ì 
- âš ï¸ **ì—…ê·¸ë ˆì´ë“œ ë³µì¡**: kubeadm upgrade ì‹œ í•¨ê»˜ ì—…ê·¸ë ˆì´ë“œ

**etcd Static Pod Manifest**:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: etcd
  namespace: kube-system
spec:
  containers:
  - name: etcd
    image: registry.k8s.io/etcd:3.5.25-0
    command:
    - etcd
    - --name=k8s-node1
    - --data-dir=/var/lib/etcd
    # ...
    volumeMounts:
    - name: etcd-data
      mountPath: /var/lib/etcd
  volumes:
  - name: etcd-data
    hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
```

---

#### (3) ë¹„êµí‘œ

| í•­ëª© | host (systemd) | kubeadm (Static Pod) |
|------|----------------|----------------------|
| **ê´€ë¦¬ ë„êµ¬** | systemctl | kubectl |
| **ì—…ê·¸ë ˆì´ë“œ** | etcdë§Œ ë³„ë„ ì—…ê·¸ë ˆì´ë“œ | kubeadm upgrade í•„ìš” |
| **ë°±ì—…/ë³µêµ¬** | systemctl stop â†’ ë°±ì—… | ë” ë³µì¡ |
| **ì¥ì•  ë³µêµ¬** | systemctl restart | Pod ì¬ì‹œì‘ |
| **ì í•© í™˜ê²½** | í”„ë¡œë•ì…˜, ìš´ì˜ í¸ì˜ì„± ì¤‘ìš” | kubeadm í†µí•© ê´€ë¦¬ ì„ í˜¸ |

---

### 4. PodDisruptionBudgetê³¼ Drain

#### (1) PDB ë™ì‘ ì›ë¦¬

**PodDisruptionBudget (PDB)**:
- âœ… **ëª©ì **: ìë°œì  ì¤‘ë‹¨(Voluntary Disruption) ì‹œ ìµœì†Œ ê°€ìš© Pod ìˆ˜ ë³´ì¥
- âœ… **ìë°œì  ì¤‘ë‹¨**: ë…¸ë“œ Drain, í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ, Pod ì‚­ì œ
- âš ï¸ **ë¹„ìë°œì  ì¤‘ë‹¨**: ë…¸ë“œ ì¥ì• , OOM, í•˜ë“œì›¨ì–´ ê³ ì¥ (PDB ì ìš© ì•ˆ ë¨)

**PDB ì˜ˆì‹œ**:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webpod
spec:
  maxUnavailable: 1  # ìµœëŒ€ 1ê°œ Podë§Œ ë™ì‹œ ì¤‘ë‹¨ ê°€ëŠ¥
  selector:
    matchLabels:
      app: webpod
```

**ë˜ëŠ”**:

```yaml
spec:
  minAvailable: 2  # ìµœì†Œ 2ê°œ PodëŠ” í•­ìƒ Running
  selector:
    matchLabels:
      app: webpod
```

---

#### (2) Drainê³¼ PDB ì¶©ëŒ

**ì‹œë‚˜ë¦¬ì˜¤**: 3ê°œ Replica Pod + maxUnavailable: 0

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      containers:
      - name: nginx
        image: nginx
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webpod
spec:
  maxUnavailable: 0  # ì–´ë–¤ Podë„ ì¤‘ë‹¨ ë¶ˆê°€
  selector:
    matchLabels:
      app: webpod
```

**Drain ì‹¤í–‰**:

```bash
kubectl drain k8s-node4 --ignore-daemonsets
```

**ê²°ê³¼**:
```
error: cannot evict pod as it would violate the pod's disruption budget.
evicting pod default/webpod-xxx
error: Cannot evict pod as it would violate the pod's disruption budget.
```

**íƒ€ì„ì•„ì›ƒ**: Drainì´ ë¬´í•œ ëŒ€ê¸° (ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ ì—†ìŒ)

---

#### (3) í•´ê²° ë°©ë²•

**ë°©ë²• 1**: PDB ì„¤ì • ë³€ê²½

```bash
kubectl edit pdb webpod
# maxUnavailable: 0 â†’ 1ë¡œ ë³€ê²½
```

**ë°©ë²• 2**: PDB ì„ì‹œ ì‚­ì œ

```bash
kubectl delete pdb webpod

# Drain ì‹¤í–‰
kubectl drain k8s-node4 --ignore-daemonsets

# PDB ì¬ìƒì„±
kubectl apply -f pdb.yaml
```

**ë°©ë²• 3**: ê°•ì œ Drain (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)

```bash
kubectl drain k8s-node4 \
  --ignore-daemonsets \
  --delete-emptydir-data \
  --force \
  --grace-period=0
```

---

#### (4) Best Practice

**í”„ë¡œë•ì…˜ í™˜ê²½**:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webpod
spec:
  minAvailable: 2  # Replica 3ê°œ ì¤‘ ìµœì†Œ 2ê°œ ìœ ì§€
  selector:
    matchLabels:
      app: webpod
```

**ë˜ëŠ”**:

```yaml
spec:
  maxUnavailable: 1  # ìµœëŒ€ 1ê°œë§Œ ë™ì‹œ ì¤‘ë‹¨ ê°€ëŠ¥
```

**ì´ìœ **:
- âœ… **ê³ ê°€ìš©ì„± ìœ ì§€**: ìµœì†Œ 2ê°œ Podê°€ í•­ìƒ ì„œë¹„ìŠ¤
- âœ… **Drain ê°€ëŠ¥**: 1ê°œì”© ì¶•ì¶œ ê°€ëŠ¥
- âœ… **Rolling Update ê°€ëŠ¥**: ì ì§„ì  ì—…ë°ì´íŠ¸

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ì¸ì¦ì„œ SAN ì¶”ê°€

#### (1) ë¬¸ì œ ìƒí™©

**ì¦ìƒ**: External LB(192.168.10.10) ì‚¬ìš© ì‹œ ì¸ì¦ì„œ ì˜¤ë¥˜

```bash
kubectl --server=https://192.168.10.10:6443 get node
```

**ì—ëŸ¬**:
```
Unable to connect to the server: x509: certificate is valid for 192.168.10.11, 192.168.10.12, 192.168.10.13, 10.233.0.1, 127.0.0.1, localhost, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local, not 192.168.10.10
```

---

#### (2) ì›ì¸

**API Server ì¸ì¦ì„œ SAN**ì— External LB IPê°€ ì—†ìŒ

```bash
ssh k8s-node1 openssl x509 -in /etc/kubernetes/ssl/apiserver.crt -text -noout | grep -A1 "Subject Alternative Name"
```

**ì¶œë ¥**:
```
X509v3 Subject Alternative Name:
    DNS:k8s-node1, DNS:kubernetes, IP Address:192.168.10.11, IP Address:10.233.0.1
```

---

#### (3) í•´ê²° ë°©ë²•

**Step 1**: ë³€ìˆ˜ ì¶”ê°€

```yaml
# inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
supplementary_addresses_in_ssl_keys:
  - 192.168.10.10
  - k8s-api-srv.admin-lb.com
```

**Step 2**: Control Plane ì¬ë°°í¬ (ì¸ì¦ì„œë§Œ)

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v cluster.yml \
  --tags "control-plane" \
  --limit kube_control_plane \
  -e kube_version="1.32.9"
```

**Step 3**: ì¸ì¦ì„œ í™•ì¸

```bash
ssh k8s-node1 openssl x509 -in /etc/kubernetes/ssl/apiserver.crt -text -noout | grep -A1 "Subject Alternative Name"
```

**ì¶œë ¥**:
```
X509v3 Subject Alternative Name:
    DNS:k8s-api-srv.admin-lb.com, DNS:k8s-node1, IP Address:192.168.10.10, IP Address:192.168.10.11
```

---

### 2. Containerd rlimits ì´ìŠˆ

#### (1) ë¬¸ì œ ìƒí™©

**ì¦ìƒ**: Nginx Proxy Pod ë¡œê·¸ì— ì—ëŸ¬

```bash
kubectl logs -n kube-system nginx-proxy-k8s-node4
```

**ì—ëŸ¬**:
```
2026/02/04 12:00:00 [alert] 1#1: setrlimit(RLIMIT_NOFILE, 130048) failed (1: Operation not permitted)
```

---

#### (2) ì›ì¸

**Containerd ê¸°ë³¸ rlimit**ì´ 65535ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ

```bash
ssh k8s-node4 cat /etc/containerd/config.toml | grep -A5 rlimits
```

**ì¶œë ¥**:
```toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  BinaryName = "/usr/local/bin/runc"
  SystemdCgroup = true

  # Default rlimits
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options.rlimits]
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options.rlimits.NOFILE]
      hard = 65535
      soft = 65535
```

---

#### (3) í•´ê²° ë°©ë²•

**Step 1**: ë³€ìˆ˜ ì¶”ê°€

```yaml
# inventory/mycluster/group_vars/all/containerd.yml
containerd_default_base_runtime_spec_patch:
  process:
    rlimits: []  # ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •í•˜ì—¬ ì œí•œ ì œê±°
```

**Step 2**: Containerd ì¬ì„¤ì •

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v cluster.yml \
  --tags "containerd" \
  --limit k8s-node4,k8s-node5 \
  -e kube_version="1.32.9"
```

**Step 3**: Nginx Proxy Pod ì¬ì‹œì‘

```bash
ssh k8s-node4 crictl rmp $(crictl pods --name nginx-proxy -q)
```

**Step 4**: ë¡œê·¸ í™•ì¸

```bash
kubectl logs -n kube-system nginx-proxy-k8s-node4
```

**ì •ìƒ ì¶œë ¥**:
```
2026/02/04 12:05:00 [notice] 1#1: using the "epoll" event method
2026/02/04 12:05:00 [notice] 1#1: nginx/1.27.3
2026/02/04 12:05:00 [notice] 1#1: start worker processes
```

---

### 3. PDBë¡œ ì¸í•œ Drain ì‹¤íŒ¨

#### (1) ë¬¸ì œ ìƒí™©

**ì¦ìƒ**: remove-node.yml ì‹¤í–‰ ì‹œ Drain íƒ€ì„ì•„ì›ƒ

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v remove-node.yml \
  -e node=k8s-node4
```

**ì—ëŸ¬**:
```
TASK [remove-node/pre-remove : Drain node] ****
fatal: [k8s-node4]: FAILED! => {
  "msg": "error: cannot evict pod as it would violate the pod's disruption budget."
}
```

---

#### (2) ì›ì¸ í™•ì¸

```bash
# PDB í™•ì¸
kubectl get pdb
```

**ì¶œë ¥**:
```
NAME     MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
webpod   N/A             0                 0                     10m
```

```bash
# PDB ìƒì„¸ í™•ì¸
kubectl describe pdb webpod
```

**ì¶œë ¥**:
```
Name:           webpod
Min available:  N/A
Max unavailable:  0
Selector:       app=webpod
Status:
  Allowed disruptions:  0
  Current:              3
  Desired:              3
  Total:                3
```

---

#### (3) í•´ê²° ë°©ë²•

**ë°©ë²• 1**: PDB ì‚­ì œ

```bash
kubectl delete pdb webpod
```

**ë°©ë²• 2**: PDB ì„¤ì • ë³€ê²½

```bash
kubectl edit pdb webpod
# maxUnavailable: 0 â†’ 1ë¡œ ë³€ê²½
```

**ë°©ë²• 3**: ê°•ì œ ì œê±° (ë¹„ì¶”ì²œ)

```bash
ansible-playbook -i inventory/mycluster/inventory.ini -v remove-node.yml \
  -e node=k8s-node4 \
  -e allow_ungraceful_removal=true \
  -e skip_confirmation=true
```

---

## ğŸ“ Week 5 í•™ìŠµ ì •ë¦¬

### 1. í•µì‹¬ ê°œë… ìš”ì•½

#### (1) Kubespray HA êµ¬ì„±

**3ê°€ì§€ API ì—”ë“œí¬ì¸íŠ¸ ì „ëµ**:
- âœ… **Case 1**: Client-Side LB (Nginx Static Pod) - ì™¸ë¶€ LB ë¶ˆí•„ìš”
- âœ… **Case 2**: External LB + Client-Side LB - ì´ì¤‘ ì¥ì•  ë³´í˜¸
- âœ… **Case 3**: External LB Only - ì¤‘ì•™ ì§‘ì¤‘ì‹ ê´€ë¦¬

**ì„ íƒ ê¸°ì¤€**:
- **On-Premise**: Case 1 (Client-Side LB)
- **Cloud**: Case 3 (External LB - AWS ELB, GCP LB)
- **í•˜ì´ë¸Œë¦¬ë“œ**: Case 2 (ì´ì¤‘ ë³´í˜¸)

---

#### (2) ë…¸ë“œ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬

**3ê°€ì§€ Playbook**:
- âœ… **scale.yml**: ë…¸ë“œ ì¶”ê°€ (ì•½ 3ë¶„)
- âœ… **remove-node.yml**: ë…¸ë“œ ì œê±° (ì•½ 2ë¶„, PDB ì—†ì„ ì‹œ 20ì´ˆ)
- âœ… **reset.yml**: í´ëŸ¬ìŠ¤í„° ì™„ì „ ì‚­ì œ (ë³µêµ¬ ë¶ˆê°€)

**ì£¼ì˜ì‚¬í•­**:
- âš ï¸ **ì²« ë²ˆì§¸ Control Plane ë…¸ë“œ ì œê±° ë¶ˆê°€**: í´ëŸ¬ìŠ¤í„° íŒŒê´´ ìœ„í—˜
- âš ï¸ **PDB í™•ì¸**: Drain ì „ PodDisruptionBudget ì„¤ì • í™•ì¸
- âš ï¸ **etcd ë°±ì—…**: ë…¸ë“œ ì œê±° ì „ etcd ìŠ¤ëƒ…ìƒ· ë°±ì—… ê¶Œì¥

---

#### (3) Kubespray ë³€ìˆ˜ ìš°ì„ ìˆœìœ„

**6ë‹¨ê³„ Override Flow**:
1. roles/*/defaults/main.yml (ê¸°ë³¸ê°’)
2. roles/*/vars/main.yml (ë‚´ë¶€ ê°•ì œ ë³€ìˆ˜)
3. **inventory/mycluster/group_vars/*.yml** (99% ì—¬ê¸°ì„œ ì¡°ì ˆ)
4. inventory/mycluster/host_vars/<node>.yml (íŠ¹ì • ë…¸ë“œë§Œ)
5. playbook vars (í”Œë ˆì´ë¶ ë¡œì»¬ ë³€ìˆ˜)
6. **--extra-vars (-e)** (ìµœìš°ì„ )

---

#### (4) etcd Deployment Type

**2ê°€ì§€ ë°°í¬ ë°©ì‹**:
- âœ… **host (systemd unit)**: ë…ë¦½ ê´€ë¦¬, ì—…ê·¸ë ˆì´ë“œ ìš©ì´ (í”„ë¡œë•ì…˜ ê¶Œì¥)
- âœ… **kubeadm (Static Pod)**: kubeadm í†µí•© ê´€ë¦¬

**í”„ë¡œë•ì…˜ ê¶Œì¥**: `etcd_deployment_type: host`

---

### 2. ì‹¤ìŠµ í™˜ê²½

**ê°€ìƒë¨¸ì‹  êµ¬ì„±**:
- **Control Plane**: 3 Nodes (k8s-node1~3)
- **Worker**: 2 Nodes (k8s-node4~5)
- **External LB**: 1 Node (admin-lb - HAProxy)

**ì†Œí”„íŠ¸ì›¨ì–´ ë²„ì „**:
- Kubernetes: v1.32.9
- Kubespray: v2.29.1
- etcd: v3.5.25
- Containerd: v2.1.5

---

### 3. ì£¼ìš” ëª…ë ¹ì–´

**ë°°í¬**:
```bash
# ì „ì²´ ë°°í¬
ansible-playbook -i inventory/mycluster/inventory.ini -v cluster.yml -e kube_version="1.32.9"

# ë…¸ë“œ ì¶”ê°€
ansible-playbook -i inventory/mycluster/inventory.ini -v scale.yml --limit=k8s-node5

# ë…¸ë“œ ì œê±°
ansible-playbook -i inventory/mycluster/inventory.ini -v remove-node.yml -e node=k8s-node5

# í´ëŸ¬ìŠ¤í„° ë¦¬ì…‹
ansible-playbook -i inventory/mycluster/inventory.ini -v reset.yml
```

**ëª¨ë‹ˆí„°ë§**:
```bash
# kube-ops-view
open "http://192.168.10.14:30000/#scale=1.5"

# HAProxy í†µê³„
open "http://192.168.10.10:9000/haproxy_stats"
```

---

### 4. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì²´í¬ë¦¬ìŠ¤íŠ¸

**ë°°í¬ ì „**:
- [ ] Swap ë¹„í™œì„±í™” í™•ì¸
- [ ] ì»¤ë„ ëª¨ë“ˆ ë¡œë“œ (overlay, br_netfilter)
- [ ] SSH Passwordless ì„¤ì •
- [ ] Firewalld/SELinux ë¹„í™œì„±í™”

**ë°°í¬ ì¤‘**:
- [ ] inventory.ini ê·¸ë£¹ ì„¤ì • í™•ì¸
- [ ] Flannel ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (flannel_interface)
- [ ] External LB ì‚¬ìš© ì‹œ SAN ì¶”ê°€

**ë°°í¬ í›„**:
- [ ] ë…¸ë“œ ìƒíƒœ í™•ì¸ (kubectl get node)
- [ ] etcd í´ëŸ¬ìŠ¤í„° í™•ì¸ (etcdctl member list)
- [ ] Control Plane Pod í™•ì¸ (kubectl get pod -n kube-system)
- [ ] CoreDNS ë™ì‘ í™•ì¸ (nslookup kubernetes.default)

**ë…¸ë“œ ì œê±° ì‹œ**:
- [ ] PDB ì„¤ì • í™•ì¸ (kubectl get pdb)
- [ ] etcd ë°±ì—… (etcdctl snapshot save)
- [ ] ì²« ë²ˆì§¸ Control Plane ë…¸ë“œê°€ ì•„ë‹Œì§€ í™•ì¸

---

### 5. Next Steps

**Week 6 Preview** (ì˜ˆìƒ):
- âœ… **Kubernetes Upgrade**: Kubesprayë¥¼ í†µí•œ í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ
- âœ… **etcd Backup & Restore**: ë°±ì—… ë° ë³µêµ¬ ì „ëµ
- âœ… **Certificate Management**: ì¸ì¦ì„œ ê°±ì‹  ë° ê´€ë¦¬
- âœ… **í”„ë¡œë•ì…˜ ì „í™˜**: Best Practice ë° ë³´ì•ˆ ê°•í™”

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Kubespray GitHub](https://github.com/kubernetes-sigs/kubespray)
- [Kubespray HA Mode ë¬¸ì„œ](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/ha-mode.md)
- [Kubespray Node Management](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md)
- [Kubeadm Reset Workflow](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/)
- [PodDisruptionBudget ê³µì‹ ë¬¸ì„œ](https://kubernetes.io/docs/tasks/run-application/configure-pdb/)

### ì†¡ì´ë ˆë‹˜ ë¸”ë¡œê·¸ ì‹œë¦¬ì¦ˆ
- [Kubespray í´ëŸ¬ìŠ¤í„° êµ¬ì„± - ì—”ë“œí¬ì¸íŠ¸ êµ¬ì„±](https://sirzzang.github.io/kubernetes/Kubernetes-Kubespray-05-00/)
- [Kubespray í´ëŸ¬ìŠ¤í„° êµ¬ì„± - ë…¸ë“œ ì¶”ê°€](https://sirzzang.github.io/kubernetes/Kubernetes-Kubespray-06-00-01/)
- [Kubespray í´ëŸ¬ìŠ¤í„° êµ¬ì„± - ë…¸ë“œ ì œê±°](https://sirzzang.github.io/kubernetes/Kubernetes-Kubespray-06-00-02/)

### ì¶”ê°€ ìë£Œ
- [HAProxy ê³µì‹ ë¬¸ì„œ](http://www.haproxy.org/)
- [Nginx TCP/UDP LoadBalancing](https://docs.nginx.com/nginx/admin-guide/load-balancer/tcp-udp-load-balancer/)
- [kube-ops-view GitHub](https://github.com/hjacobs/kube-ops-view)
- [Vagrant ê³µì‹ ë¬¸ì„œ](https://www.vagrantup.com/docs)

---

**ìŠ¤í„°ë””**: CloudNet@ ê°€ì‹œë‹¤ë‹˜ ì£¼ê´€ - Kubernetes Deploy ìŠ¤í„°ë””
